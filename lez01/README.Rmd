---
title: "lez01: Introduzione al corso - L'analisi dei dati - Approccio alle statistiche"
output: 
        github_document:
                toc: TRUE
                toc_depth: 3
---

## Perché stiamo facendo questo corso?
##### Cosa ci si aspetta dagli studenti e chi lo dovrebbe insegnare:
| Materia | Competenza |
| ------- | ---------- |
| Statistica | Quali dati raccogliere |
| **Materie specifiche** | **Come si raccolgono** |
| Data Management | Digitalizzare i dati raccolti |
| Data Carpentry | Elaborare praticamente i dati |
| Statistica descrittiva | Riassumere i contenuti |
| Statistica inferenziale | Individuare i legami causali |



## Replicabilità

##### La scienza come somma di test riproducibili
Per qualsiasi analisi scientifica il punto principale è la possibilità di essere ripetuta (e fornire risultati analoghi) da altri operatori in possesso delle adeguate abilità. In particolare è necessario, in uno studio di tipo scientifico, fornire tutti i dettagli necessari a replicare le analisi svolte.  
Questa funzione è normalmente svolta dalla sezione "Materiali e Metodi" degli articoli scientifici e delle tesi.

##### Materiali e Metodi VS Analisi Point & Click
Se di un'analisi di laboratorio è possibile specificare passo passo tutte le operaizoni svolte, i reagenti utilizzati, ed i macchinari, questo diventa quasi impossibile per le elaborazioni dei dati:
Il numero di passi, dall'apertura dei file al settaggio delle impostazioni dei grafici, rende impossibile una tracciatura univoca del processo, e quindi la riproduzione dei risultati ottenuti.
Se questa sembra un'esagerazione o una pignoleria, si può provare a riprendere in mano una qualsiasi elaboraizone di 3 anni prima e chiedere a chi l'ha fatta di ripeterla, e poi ridere.
Il problema è spesso generato dalla complessità crescente sia dei dati, sia delle analisi che su questi dati vengono svolte.

##### Come si risolve il problema?
Automatizzando tutti i processi, dall'inizio alla fine.  
Questo però prevede l'acquisizione di una serie di competenze nuove e di familiarità con una serie di strumenti che non fanno parte storicamente del nostro bagaglio.  
Queste competenze possono però essere mutuate da chi si è scontrato con gli stessi problemi decine di anni prima, cioè l'informatica. Tutto sta nel trovare degli strumenti che rendano un po' meno problematica l'adozione delle soluzioni che l'informatica ha adottato già da decenni.

#### FAQ
##### Perché non EXCEL
* I dati con cui stiamo lavorando sono troppi per il programma 
* la complessità delle operazioni rende le cose troppo complicate
* Il programma, *per quanto non ci pensiamo*, non è gratuito
* Le analisi più complesse sono semplicemente infattibili, richiedendo l'appoggio ad altri software
* I passaggi (selezionare, spostare, copiare e incollare) sono un incubo da automatizzare
* Se non automatizzati rendono il tutto assolutamente irriproducibile (e spesso non modificabile)

##### Perché non Python
* Questa è "quasi" solo una scelta di gusti
* Python, per quanto ancora più versatile, e proprio per quel motivo, richiede un approccio ancora più orientato all'informatica
* Idealmente *potrebbe essere una buona scelta imparare entrambi i linguaggi*, anche se il settore delle scienze applicate non si giova particolarmente della versatilità di Python

## Collaborazione
##### Il primo dei problemi: io non parlo sloveno, tantomeno lo leggo
La collaborazione, nel mondo accademico ma non solo, passa prima di tutto da un linguaggio comune, che non dovrebbe mai essere dato per scontato: *fate un punto d'onore di utilizzare solo l'inglese per ogni cosa relativa ai vostri studi*, qualsiasi materiale prodotto, qualsiasi dato raccolto. E ringraziate il cielo che la lingua della comunità scientifica non sia più il latino.

##### 2: le mille versioni di un file
Anche lavorando da soli ci si può trovare a salvare decine di versioni dello stesso file di testo, per segmentarlo in diversi file quando cresce troppo, per finre con dei raccapriccianti archivi .zip di intere cartelle. Questo, quando si collabora in più persone allo stesso progetto, aumenta in modo esponenziale, mentre si aggiunge un altro problema:

##### 3: chi sta lavorando a cosa
Va tutto bene finché non si scopre di aver lavorato due giorni su un file che qualcun altro ha nel frattempo reso completamente irrilevante.

Questo è uno dei motivi per cui si **consiglia fortemente** <sup id="a1">[1](#f1)</sup> l'utilizzo del version control anche per i file che non contengono codice, e si ricorda che **NON È POSSIBILE METTERE SOTTO VERSION CONTROL FILE DI WORD**. Il formato Markdown, in particolare quello di R, permette in modo semplice di generare dei PDF estremamente funzionali e al contempo non impazzire con LaTeX.

## Manutenzione
##### Tutto il processo deve essere aggiornabile
Aggiornabile con il minimo sforzo, il che richiede che tutto, **tutto**, sia scomposto nelle più piccole unità funzionali possibili, in modo da poterle rimpiazzare, correggere, aggiornare, espandere, senza dover rimettere le mani all'intero progetto.

##### LA DOCUMENTAZIONE
Ritornare su un progetto a distanza di settimane (quando non sono anni), può essere un'esperienza illuminante: spesso risulta più semplice ricostruire tutto da capo che non effettuare una piccola variazione. Il primo dei motivi per cui questo succede è l'assenz di documentazione: cioé di file dove sono spiegati tutti i termini, le abbreviazioni, le unità di misura, le scelte che sono state fatte.
Mentre buone pratiche di scrittura (come l'identazione del codice, la scelta dei nomi per le variabili, ecc) si apprendono fondamentalmente con la pratica, una documentazione rigorosa può essere autoimposta attraverso alcune tecniche più avanzate, in primi un Versioning costante (ed altre che al momento esulano dagli scopi di questo corso, in particolare la creazione di `package` con RStudio).

## L'analisi dei dati
L'analisi dei dati (alla fin fine il centro di tutto questo corso) sta diventando sempre più una disciplina a se, all'interno dei differenti rami delle scienze: la mole di dati aumenta costantemente, le metodologie e le tecnologie per ottenerne informazioni idem. Tutto questo al momento va sotto il nome di *Data Science*, che comprende diverse fasi, ognuna a quanto pare con un suo nome e i suoi specialisti: *Data Harvesting*, *Data Carpentry*, *Machine Learning*, *Data Visualization*.

## Statistica descrittiva

##Statistica inferenziale

##Statistica predittiva

##Hypotesis Testing


---

[Syllabus](../README.md) <---> [lez02](/lez02/)

---

#### Materiale
[GitHub - Fork e Pull Request for dummies](https://www.chronicle.com/blogs/profhacker/forks-and-pull-requests-in-github/47753)

#### Note

<i id="f1" >1</i> [Eccellente risorsa sul versioning in (anche per i file di solo testo) e sull'utilizzo di GitHub per principianti (e non)](https://programminghistorian.org/lessons/getting-started-with-github-desktop#why-version-control-text-documents) [↩](#a1)